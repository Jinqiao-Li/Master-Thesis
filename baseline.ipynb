{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fbdd04-a930-4538-805b-d75927f0a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required packages\n",
    "import sys\n",
    "import os\n",
    "# change according to the status of GPU\n",
    "# this command must before import pytorch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,4,5'  # setting the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297bb43d-3df0-4d68-947b-3f06d428ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "\n",
    "from transformers import AutoModel, BertForSequenceClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, RandomSampler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,recall_score,precision_score\n",
    "# metrics for multi-label classification\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35cf317-847e-4c20-b9c5-60e5463b861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/jinqli/anaconda3/envs/new_env/bin/python\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the running environment\n",
    "print(sys.executable)\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab60f9-8437-4bf8-8bbd-7e185aa47da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the prepared German data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a1097-6e72-4d50-82c5-1317a90ef5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of records in tasks to DWAs(de): 23543\n",
      "unique types of GWA 37\n",
      "unique types of IWA 332\n",
      "unique types of DWA 2085\n"
     ]
    }
   ],
   "source": [
    "de_path = '/srv/scratch2/jinq/taskontology/task_to_GWA_IWA_DWA_DE.csv'\n",
    "\n",
    "data_de = pd.read_csv(de_path, index_col=0)\n",
    "print('numbers of records in tasks to DWAs(de):', data_de.shape[0])\n",
    "print('unique types of GWA', data_de['GWA Title'].nunique())\n",
    "print('unique types of IWA', data_de['IWA Title'].nunique())\n",
    "print('unique types of DWA', data_de['DWA Title'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a2d62-0ba8-454b-bcb5-32a06f1fae68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5685ef35-af00-484a-b3e7-08528626320e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbc131-a109-4c92-9b27-d1179b7d6b7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9f8419f-63c2-4c3f-a77d-ed95774e7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path, tokenizer, y_level):\n",
    "    # load the data\n",
    "    dataset = load_dataset('csv', data_files=data_path,split='train')\n",
    "    print('Size of the dataset: ',len(dataset))\n",
    "\n",
    "    # encoding tasks\n",
    "    encoded_data = [tokenizer(item['Task_de'], \n",
    "                              return_tensors=\"pt\", padding='max_length', truncation=True, \n",
    "                              max_length=128, is_split_into_words=True) for item in dataset]\n",
    "\n",
    "    # encoding labels\n",
    "    y_encoded = LabelEncoder().fit_transform(dataset[y_level])\n",
    "    # print('Encoded labels: ', y_encoded)\n",
    "\n",
    "    # Zipping the tasks and the labels(GWA title) together again\n",
    "    for enc_item, item in zip(encoded_data, y_encoded):\n",
    "        enc_item['labels'] = torch.tensor(item)\n",
    "\n",
    "    for item in encoded_data:\n",
    "        for key in item:\n",
    "            item[key] = torch.squeeze(item[key])\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def split_data(encoded_data):\n",
    "    # split into train, validation and test      \n",
    "    train_set, test_val_set = train_test_split(encoded_data, test_size=0.2)\n",
    "    test_set, val_set = train_test_split(test_val_set, test_size=0.5)\n",
    "\n",
    "    print('length of the training set: ', len(train_set))\n",
    "    print('length of the test set: ',len(test_set))\n",
    "    print('length of the val set: ',len(val_set))\n",
    "    \n",
    "    # details in the dataset\n",
    "    #for key, val in test_set[3].items():\n",
    "    #    print(f'key: {key}, dimensions: {val.size()}')\n",
    "    \n",
    "    return train_set,val_set,test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d95ceaf-8144-4b68-a507-5ef8d83f4cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2bd0bdea745cdb59\n",
      "Found cached dataset csv (/home/user/jinqli/.cache/huggingface/datasets/csv/default-2bd0bdea745cdb59/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  23543\n",
      "length of the training set:  18834\n",
      "length of the test set:  2354\n",
      "length of the val set:  2355\n"
     ]
    }
   ],
   "source": [
    "encoded_data = encode_data(de_path, tokenizer, 'GWA Title')\n",
    "# print('An example of zipped task and label: \\n', encoded_data.__getitem__(10)) \n",
    "train_set,val_set,test_set = split_data(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac4c96-0716-48d8-9be6-3c2237c9236e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af370ee7-dbc6-477c-8dc0-c3997f8d2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for evaluation\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    balanced_accuracy = balanced_accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds,average='micro')\n",
    "    recall = recall_score(labels, preds,average='micro')\n",
    "    precision = precision_score(labels, preds,average='micro')\n",
    "    hamming_loss = hamming_loss(labels, preds,average='micro')\n",
    "\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'balanced_accuracy': balanced_accuracy,\n",
    "      'f1_score': f1,\n",
    "      'recall': recall,\n",
    "      'precision': precision,\n",
    "      'hamming_loss': hamming_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea350d2-686f-4ee7-a103-5c3bf728c45c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930562-847a-4f90-a254-ad59b3bc9692",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea13aa6-0f2c-4f8c-bce0-2d591de43408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "gwa_labels=37\n",
    "iwa_labels=332\n",
    "dwa_labels=2085\n",
    "\n",
    "hidden_dropout_prob = 0.3\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-2\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f766db5-2067-4c40-a4e4-924ca0d22979",
   "metadata": {
    "tags": []
   },
   "source": [
    "### gbert-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e5b078-07ed-4e2c-859c-2d205d581d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "\n",
    "german_model = \"deepset/gbert-base\" \n",
    "Bertmodel =  BertForSequenceClassification.from_pretrained(german_model,num_labels=gwa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cf83fb-a9d3-4055-bd91-ba48a34e2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='./gbert_results',\n",
    "    logging_dir='./gbert_logs',\n",
    "    metric_for_best_model = 'hamming_loss',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_gbert = Trainer(\n",
    "    model=Bertmodel,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e1394f-dec8-430d-a7f1-2a2e557b93c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 21:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>3.036555</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.060654</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>6.456400</td>\n",
       "      <td>364.753000</td>\n",
       "      <td>2.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>2.625865</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>0.136418</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>6.458500</td>\n",
       "      <td>364.634000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>2.369962</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.211314</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>6.611900</td>\n",
       "      <td>356.177000</td>\n",
       "      <td>2.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>2.232846</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.233082</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>6.476000</td>\n",
       "      <td>363.648000</td>\n",
       "      <td>2.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>2.107863</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.267190</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>6.448000</td>\n",
       "      <td>365.230000</td>\n",
       "      <td>2.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>2.048754</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.285829</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>6.604500</td>\n",
       "      <td>356.578000</td>\n",
       "      <td>2.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>1.987307</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.299866</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>6.604000</td>\n",
       "      <td>356.604000</td>\n",
       "      <td>2.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>1.961134</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>0.300642</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>6.458300</td>\n",
       "      <td>364.648000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>1.948031</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.314623</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>6.460700</td>\n",
       "      <td>364.510000</td>\n",
       "      <td>2.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>1.908834</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.325425</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>6.558500</td>\n",
       "      <td>359.076000</td>\n",
       "      <td>2.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>1.897575</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.330760</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>6.458900</td>\n",
       "      <td>364.611000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>1.911257</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.319689</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>6.550200</td>\n",
       "      <td>359.530000</td>\n",
       "      <td>2.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>1.897509</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.327784</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>6.462200</td>\n",
       "      <td>364.428000</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>1.893690</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.336353</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>6.452400</td>\n",
       "      <td>364.983000</td>\n",
       "      <td>2.945000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./gbert_results/checkpoint-500\n",
      "Configuration saved in ./gbert_results/checkpoint-500/config.json\n",
      "Model weights saved in ./gbert_results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./gbert_results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./gbert_results/checkpoint-500/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./gbert_results/checkpoint-1000\n",
      "Configuration saved in ./gbert_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./gbert_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./gbert_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./gbert_results/checkpoint-1000/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./gbert_results/checkpoint-1000 (score: 0.505307855626327).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1480, training_loss=1.996760724041913, metrics={'train_runtime': 1298.9833, 'train_samples_per_second': 144.99, 'train_steps_per_second': 1.139, 'total_flos': 1.23924771650304e+16, 'train_loss': 1.996760724041913, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_gbert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0131182-9da4-4f23-b6d8-bacd1ef1e594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### job modetrainer_multibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "678dfd63-0266-4654-9e06-24a53e331977",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/agne/jobGBERT/resolve/main/config.json from cache at /home/user/jinqli/.cache/huggingface/transformers/d2d46e69040501cda16aa4d54a8a989fe5119ab3efd9364107329b4473dc8cbc.6efb02902914f07db4531e133a3c97080eeae52c7268a17915c9327e16cfaf1c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"agne/jobGBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/agne/jobGBERT/resolve/main/pytorch_model.bin from cache at /home/user/jinqli/.cache/huggingface/transformers/bf7e1c249aaf4fb8059ae70ca774d6b4eda1421bb2875d98c1b8c5ea5d5238e2.b132d77f68b8e2275c91b5342968a8ec7c9b45cb34bcb2e75165e872342c5dfd\n",
      "Some weights of the model checkpoint at agne/jobGBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at agne/jobGBERT and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "job_model = \"agne/jobGBERT\"\n",
    "Jobmodel = BertForSequenceClassification.from_pretrained(job_model,num_labels=gwa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81431f04-b18a-4fb5-b8a3-312bcc4ee876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='./job_results',\n",
    "    logging_dir='./job_logs',\n",
    "    metric_for_best_model = 'f1_score',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_job = Trainer(\n",
    "    model=Jobmodel,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "282ef0ff-e6a7-414a-857c-8990d329cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 21:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.205520</td>\n",
       "      <td>2.983229</td>\n",
       "      <td>0.205520</td>\n",
       "      <td>0.066974</td>\n",
       "      <td>0.205520</td>\n",
       "      <td>0.205520</td>\n",
       "      <td>0.205520</td>\n",
       "      <td>6.454300</td>\n",
       "      <td>364.871000</td>\n",
       "      <td>2.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.308705</td>\n",
       "      <td>2.665582</td>\n",
       "      <td>0.308705</td>\n",
       "      <td>0.124907</td>\n",
       "      <td>0.308705</td>\n",
       "      <td>0.308705</td>\n",
       "      <td>0.308705</td>\n",
       "      <td>6.450500</td>\n",
       "      <td>365.086000</td>\n",
       "      <td>2.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.383439</td>\n",
       "      <td>2.378019</td>\n",
       "      <td>0.383439</td>\n",
       "      <td>0.183969</td>\n",
       "      <td>0.383439</td>\n",
       "      <td>0.383439</td>\n",
       "      <td>0.383439</td>\n",
       "      <td>6.673400</td>\n",
       "      <td>352.892000</td>\n",
       "      <td>2.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424204</td>\n",
       "      <td>2.226835</td>\n",
       "      <td>0.424204</td>\n",
       "      <td>0.224458</td>\n",
       "      <td>0.424204</td>\n",
       "      <td>0.424204</td>\n",
       "      <td>0.424204</td>\n",
       "      <td>6.445100</td>\n",
       "      <td>365.394000</td>\n",
       "      <td>2.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>0.452654</td>\n",
       "      <td>2.125789</td>\n",
       "      <td>0.452654</td>\n",
       "      <td>0.255776</td>\n",
       "      <td>0.452654</td>\n",
       "      <td>0.452654</td>\n",
       "      <td>0.452654</td>\n",
       "      <td>6.654700</td>\n",
       "      <td>353.885000</td>\n",
       "      <td>2.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>2.041583</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.260881</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>6.444300</td>\n",
       "      <td>365.438000</td>\n",
       "      <td>2.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>0.464968</td>\n",
       "      <td>2.002359</td>\n",
       "      <td>0.464968</td>\n",
       "      <td>0.271326</td>\n",
       "      <td>0.464968</td>\n",
       "      <td>0.464968</td>\n",
       "      <td>0.464968</td>\n",
       "      <td>6.667800</td>\n",
       "      <td>353.189000</td>\n",
       "      <td>2.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>0.482803</td>\n",
       "      <td>1.956388</td>\n",
       "      <td>0.482803</td>\n",
       "      <td>0.284735</td>\n",
       "      <td>0.482803</td>\n",
       "      <td>0.482803</td>\n",
       "      <td>0.482803</td>\n",
       "      <td>6.512300</td>\n",
       "      <td>361.622000</td>\n",
       "      <td>2.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.617300</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>1.914729</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.296502</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>6.457500</td>\n",
       "      <td>364.695000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.841900</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>1.899420</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>0.294387</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>6.599600</td>\n",
       "      <td>356.838000</td>\n",
       "      <td>2.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.841900</td>\n",
       "      <td>0.493843</td>\n",
       "      <td>1.895702</td>\n",
       "      <td>0.493843</td>\n",
       "      <td>0.293199</td>\n",
       "      <td>0.493843</td>\n",
       "      <td>0.493843</td>\n",
       "      <td>0.493843</td>\n",
       "      <td>6.446300</td>\n",
       "      <td>365.327000</td>\n",
       "      <td>2.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.841900</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>1.878885</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.304098</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>0.497665</td>\n",
       "      <td>6.437600</td>\n",
       "      <td>365.821000</td>\n",
       "      <td>2.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.841900</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>1.872796</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>0.304094</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>0.498514</td>\n",
       "      <td>6.631200</td>\n",
       "      <td>355.139000</td>\n",
       "      <td>2.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.841900</td>\n",
       "      <td>0.498089</td>\n",
       "      <td>1.875437</td>\n",
       "      <td>0.498089</td>\n",
       "      <td>0.303933</td>\n",
       "      <td>0.498089</td>\n",
       "      <td>0.498089</td>\n",
       "      <td>0.498089</td>\n",
       "      <td>6.447000</td>\n",
       "      <td>365.286000</td>\n",
       "      <td>2.947000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./job_results/checkpoint-500\n",
      "Configuration saved in ./job_results/checkpoint-500/config.json\n",
      "Model weights saved in ./job_results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./job_results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./job_results/checkpoint-500/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./job_results/checkpoint-1000\n",
      "Configuration saved in ./job_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./job_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./job_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./job_results/checkpoint-1000/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./job_results/checkpoint-1000 (score: 0.48874734607218684).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1480, training_loss=2.012536786053632, metrics={'train_runtime': 1284.6465, 'train_samples_per_second': 146.608, 'train_steps_per_second': 1.152, 'total_flos': 1.23924771650304e+16, 'train_loss': 2.012536786053632, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_job.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0c599-1e95-4c4a-9d49-ea71fdba9eb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Multilingual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb2856a-de6e-446d-b6e5-b6d428ed07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465d4139-6061-4440-b809-ac4ab0a48f1f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/user/jinqli/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /home/user/jinqli/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /home/user/jinqli/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/user/jinqli/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/user/jinqli/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/user/jinqli/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multi_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "multilingual_model = \"bert-base-multilingual-cased\" \n",
    "Multi_Bertmodel =  BertForSequenceClassification.from_pretrained(multilingual_model, num_labels=gwa_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee1e3d78-0096-493f-9665-bdb9ca939b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2bd0bdea745cdb59\n",
      "Found cached dataset csv (/home/user/jinqli/.cache/huggingface/datasets/csv/default-2bd0bdea745cdb59/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  23543\n",
      "length of the training set:  18834\n",
      "length of the test set:  2354\n",
      "length of the val set:  2355\n"
     ]
    }
   ],
   "source": [
    "encoded_data = encode_data(de_path, multi_tokenizer, 'GWA Title')\n",
    "train_set,val_set,test_set = split_data(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3de4361-3232-47a3-97c8-c179b88905af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='./multibert_results',\n",
    "    logging_dir='./multibert_logs',\n",
    "    metric_for_best_model = 'f1_score',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_multibert = Trainer(\n",
    "    model=Multi_Bertmodel,\n",
    "    tokenizer=multi_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d41198-361d-458f-ba57-cc3919cd6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 25:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>2.714472</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.088463</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>7.873000</td>\n",
       "      <td>299.124000</td>\n",
       "      <td>2.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>2.196005</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>0.216565</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>7.863600</td>\n",
       "      <td>299.483000</td>\n",
       "      <td>2.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>1.986812</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>0.257020</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>8.154400</td>\n",
       "      <td>288.803000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>1.857215</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.304660</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>7.867000</td>\n",
       "      <td>299.353000</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>1.803832</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.331783</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>7.862100</td>\n",
       "      <td>299.539000</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>1.749402</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.364005</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>7.868400</td>\n",
       "      <td>299.299000</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>1.734157</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>7.848200</td>\n",
       "      <td>300.068000</td>\n",
       "      <td>2.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>1.729213</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.382370</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>8.090700</td>\n",
       "      <td>291.076000</td>\n",
       "      <td>2.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>1.736151</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.387682</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>7.850500</td>\n",
       "      <td>299.980000</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>1.729934</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.393352</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>7.856900</td>\n",
       "      <td>299.737000</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>1.737821</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.402857</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>7.851300</td>\n",
       "      <td>299.950000</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>1.729427</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>0.407679</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>7.878000</td>\n",
       "      <td>298.933000</td>\n",
       "      <td>2.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>1.741030</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.408983</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>7.886700</td>\n",
       "      <td>298.604000</td>\n",
       "      <td>2.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>1.740551</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.401061</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>8.171700</td>\n",
       "      <td>288.191000</td>\n",
       "      <td>2.325000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./multibert_results/checkpoint-500\n",
      "Configuration saved in ./multibert_results/checkpoint-500/config.json\n",
      "Model weights saved in ./multibert_results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./multibert_results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./multibert_results/checkpoint-500/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./multibert_results/checkpoint-1000\n",
      "Configuration saved in ./multibert_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./multibert_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./multibert_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./multibert_results/checkpoint-1000/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./multibert_results/checkpoint-1000 (score: 0.5286624203821656).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1480, training_loss=1.570254846521326, metrics={'train_runtime': 1516.2553, 'train_samples_per_second': 124.214, 'train_steps_per_second': 0.976, 'total_flos': 1.23924771650304e+16, 'train_loss': 1.570254846521326, 'epoch': 10.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_multibert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bc91661-3593-47bf-a4b7-6375995014b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./multibert_results/checkpoint-1000\n",
      "Configuration saved in ./multibert_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./multibert_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./multibert_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./multibert_results/checkpoint-1000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "best_ckpt_path = trainer_multibert.state.best_model_checkpoint\n",
    "trainer_multibert.save_model(best_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f57c72-69b2-4643-a044-ccb6086f4769",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08f7612f-9777-414f-a9c6-e67e86e7a7be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8680411577224731, 'eval_accuracy': 0.5040339702760085, 'eval_balanced_accuracy': 0.3215903417617457, 'eval_f1_score': 0.5040339702760085, 'eval_recall': 0.5040339702760085, 'eval_precision': 0.5040339702760085, 'eval_hamming_loss': 0.5040339702760085, 'eval_runtime': 6.2722, 'eval_samples_per_second': 375.466, 'eval_steps_per_second': 3.029, 'epoch': 8.78}\n"
     ]
    }
   ],
   "source": [
    "## Comparing these two models\n",
    "metrics_gbert=trainer_gbert.evaluate()\n",
    "print(metrics_gbert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6de72fda-4a43-4ea1-bb25-44a149c4bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8994202613830566, 'eval_accuracy': 0.48874734607218684, 'eval_balanced_accuracy': 0.29438654919459367, 'eval_f1_score': 0.48874734607218684, 'eval_recall': 0.48874734607218684, 'eval_precision': 0.48874734607218684, 'eval_hamming_loss': 0.48874734607218684, 'eval_runtime': 6.2803, 'eval_samples_per_second': 374.979, 'eval_steps_per_second': 3.025, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_job=trainer_job.evaluate()\n",
    "print(metrics_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4777085e-0416-406d-a727-966055282bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7299339771270752, 'eval_accuracy': 0.5286624203821656, 'eval_balanced_accuracy': 0.3933516082476829, 'eval_f1_score': 0.5286624203821656, 'eval_recall': 0.5286624203821656, 'eval_precision': 0.5286624203821656, 'eval_hamming_loss': 0.5286624203821656, 'eval_runtime': 7.4651, 'eval_samples_per_second': 315.469, 'eval_steps_per_second': 2.545, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "metrics_multibert=trainer_multibert.evaluate()\n",
    "print(metrics_multibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa200f3a-6989-4f2c-ab56-a318fdfa75c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
