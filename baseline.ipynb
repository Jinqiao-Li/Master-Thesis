{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fbdd04-a930-4538-805b-d75927f0a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required packages\n",
    "import sys\n",
    "import os\n",
    "# change according to the status of GPU\n",
    "# this command must before import pytorch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,4,5'  # setting the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297bb43d-3df0-4d68-947b-3f06d428ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "\n",
    "from transformers import AutoModel, BertForSequenceClassification, BertConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, RandomSampler\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,recall_score,precision_score\n",
    "# metrics for multi-label classification\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35cf317-847e-4c20-b9c5-60e5463b861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/jinqli/anaconda3/envs/new_env/bin/python\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the running environment\n",
    "print(sys.executable)\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab60f9-8437-4bf8-8bbd-7e185aa47da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the prepared German data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a1097-6e72-4d50-82c5-1317a90ef5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers of records in tasks to DWAs(de): 23543\n",
      "unique types of GWA 37\n",
      "unique types of IWA 332\n",
      "unique types of DWA 2085\n"
     ]
    }
   ],
   "source": [
    "de_path = '/srv/scratch2/jinq/taskontology/task_to_GWA_IWA_DWA_DE.csv'\n",
    "\n",
    "data_de = pd.read_csv(de_path, index_col=0)\n",
    "print('numbers of records in tasks to DWAs(de):', data_de.shape[0])\n",
    "print('unique types of GWA', data_de['GWA Title'].nunique())\n",
    "print('unique types of IWA', data_de['IWA Title'].nunique())\n",
    "print('unique types of DWA', data_de['DWA Title'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cbc131-a109-4c92-9b27-d1179b7d6b7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f8419f-63c2-4c3f-a77d-ed95774e7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path, tokenizer, y_level):\n",
    "    # load the data\n",
    "    dataset = load_dataset('csv', data_files=data_path,split='train')\n",
    "    print('Size of the dataset: ',len(dataset))\n",
    "\n",
    "    # encoding tasks\n",
    "    encoded_data = [tokenizer(item['Task_de'], \n",
    "                              return_tensors=\"pt\", padding='max_length', truncation=True, \n",
    "                              max_length=128, is_split_into_words=True) for item in dataset]\n",
    "\n",
    "\n",
    "    # encoding labels\n",
    "    y_encoded = LabelEncoder().fit_transform(dataset[y_level])\n",
    "    # print('Encoded labels: ', y_encoded)\n",
    "\n",
    "    # Zipping the tasks and the labels(GWA title) together again\n",
    "    for enc_item, item in zip(encoded_data, y_encoded):\n",
    "        enc_item['labels'] = torch.tensor(item)\n",
    "\n",
    "    for item in encoded_data:\n",
    "        for key in item:\n",
    "            item[key] = torch.squeeze(item[key])\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "def split_data(encoded_data):\n",
    "    # split into train, validation and test      \n",
    "    train_set, test_val_set = train_test_split(encoded_data, test_size=0.2)\n",
    "    test_set, val_set = train_test_split(test_val_set, test_size=0.5)\n",
    "\n",
    "    print('length of the training set: ', len(train_set))\n",
    "    print('length of the test set: ',len(test_set))\n",
    "    print('length of the val set: ',len(val_set))\n",
    "    \n",
    "    # details in the dataset\n",
    "    #for key, val in test_set[3].items():\n",
    "    #    print(f'key: {key}, dimensions: {val.size()}')\n",
    "    \n",
    "    return train_set,val_set,test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac4c96-0716-48d8-9be6-3c2237c9236e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af370ee7-dbc6-477c-8dc0-c3997f8d2c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics for evaluation\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    balanced_accuracy = balanced_accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds,average='micro')\n",
    "    recall = recall_score(labels, preds,average='micro')\n",
    "    precision = precision_score(labels, preds,average='micro')\n",
    "    hamming_loss = hamming_loss(labels, preds,average='micro')\n",
    "\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'balanced_accuracy': balanced_accuracy,\n",
    "      'f1_score': f1,\n",
    "      'recall': recall,\n",
    "      'precision': precision,\n",
    "      'hamming_loss': hamming_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea350d2-686f-4ee7-a103-5c3bf728c45c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930562-847a-4f90-a254-ad59b3bc9692",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea13aa6-0f2c-4f8c-bce0-2d591de43408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "gwa_labels=37\n",
    "iwa_labels=332\n",
    "dwa_labels=2085\n",
    "\n",
    "hidden_dropout_prob = 0.3\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-2\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f766db5-2067-4c40-a4e4-924ca0d22979",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. gbert-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e5b078-07ed-4e2c-859c-2d205d581d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True) \n",
    "\n",
    "# load the model\n",
    "german_model = \"deepset/gbert-base\" \n",
    "Bertmodel =  BertForSequenceClassification.from_pretrained(german_model,num_labels=gwa_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9df2d-a694-4d79-b3a0-d2736a4eb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding data with corresponding tokenizer\n",
    "encoded_data = encode_data(de_path, tokenizer, 'GWA Title')\n",
    "\n",
    "# print('An example of zipped task and label: \\n', encoded_data.__getitem__(10)) \n",
    "train_set,val_set,test_set = split_data(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cf83fb-a9d3-4055-bd91-ba48a34e2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='trained_models/gbert_results',\n",
    "    logging_dir='trained_models/gbert_logs',\n",
    "    metric_for_best_model = 'hamming_loss',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_gbert = Trainer(\n",
    "    model=Bertmodel,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e1394f-dec8-430d-a7f1-2a2e557b93c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 21:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>3.036555</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.060654</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>6.456400</td>\n",
       "      <td>364.753000</td>\n",
       "      <td>2.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>2.625865</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>0.136418</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>0.301486</td>\n",
       "      <td>6.458500</td>\n",
       "      <td>364.634000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>2.369962</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.211314</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>6.611900</td>\n",
       "      <td>356.177000</td>\n",
       "      <td>2.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>2.232846</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.233082</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>0.417834</td>\n",
       "      <td>6.476000</td>\n",
       "      <td>363.648000</td>\n",
       "      <td>2.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>2.107863</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.267190</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>0.451380</td>\n",
       "      <td>6.448000</td>\n",
       "      <td>365.230000</td>\n",
       "      <td>2.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>2.048754</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.285829</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.467516</td>\n",
       "      <td>6.604500</td>\n",
       "      <td>356.578000</td>\n",
       "      <td>2.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>1.987307</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.299866</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>0.481529</td>\n",
       "      <td>6.604000</td>\n",
       "      <td>356.604000</td>\n",
       "      <td>2.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>1.961134</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>0.300642</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>0.484926</td>\n",
       "      <td>6.458300</td>\n",
       "      <td>364.648000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.610700</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>1.948031</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.314623</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>0.491295</td>\n",
       "      <td>6.460700</td>\n",
       "      <td>364.510000</td>\n",
       "      <td>2.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>1.908834</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.325425</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>0.505308</td>\n",
       "      <td>6.558500</td>\n",
       "      <td>359.076000</td>\n",
       "      <td>2.897000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>1.897575</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.330760</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>6.458900</td>\n",
       "      <td>364.611000</td>\n",
       "      <td>2.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>1.911257</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.319689</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>6.550200</td>\n",
       "      <td>359.530000</td>\n",
       "      <td>2.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>1.897509</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.327784</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>0.506157</td>\n",
       "      <td>6.462200</td>\n",
       "      <td>364.428000</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.819900</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>1.893690</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.336353</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>6.452400</td>\n",
       "      <td>364.983000</td>\n",
       "      <td>2.945000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./gbert_results/checkpoint-500\n",
      "Configuration saved in ./gbert_results/checkpoint-500/config.json\n",
      "Model weights saved in ./gbert_results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./gbert_results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./gbert_results/checkpoint-500/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./gbert_results/checkpoint-1000\n",
      "Configuration saved in ./gbert_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./gbert_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./gbert_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./gbert_results/checkpoint-1000/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./gbert_results/checkpoint-1000 (score: 0.505307855626327).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1480, training_loss=1.996760724041913, metrics={'train_runtime': 1298.9833, 'train_samples_per_second': 144.99, 'train_steps_per_second': 1.139, 'total_flos': 1.23924771650304e+16, 'train_loss': 1.996760724041913, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_gbert.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0131182-9da4-4f23-b6d8-bacd1ef1e594",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. job gbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "678dfd63-0266-4654-9e06-24a53e331977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at agne/jobGBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at agne/jobGBERT and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True) \n",
    "\n",
    "job_model = \"agne/jobGBERT\"\n",
    "Jobmodel = BertForSequenceClassification.from_pretrained(job_model,num_labels=gwa_labels,problem_type='multi_label_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f07eab91-991b-4655-b01c-2cb0824be184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2bd0bdea745cdb59\n",
      "Found cached dataset csv (/home/user/jinqli/.cache/huggingface/datasets/csv/default-2bd0bdea745cdb59/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  23543\n",
      "length of the training set:  18834\n",
      "length of the test set:  2354\n",
      "length of the val set:  2355\n"
     ]
    }
   ],
   "source": [
    "# encoding data with corresponding tokenizer\n",
    "encoded_data = encode_data(de_path, tokenizer_bert, 'GWA Title')\n",
    "\n",
    "# split data into train, test validation\n",
    "train_set,val_set,test_set = split_data(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f00da68-7592-45fe-bc62-844896c77551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,   164,   112,   243,  3169,  1643,  1197, 17176, 14703,  2118,\n",
       "         5576,  9954,  1193,  2217,  3262, 11336,  9817,  1116, 12198,  9022,\n",
       "        17761,  1424,   117,   144,  6420,  5745,  1424,   184,  2692,  4167,\n",
       "          268, 15475,  2227, 26567,  1179, 17129, 17030,  1377,  5576, 18653,\n",
       "        20201, 10486, 24356,  3262,   229,  9824, 25821, 23199,   143, 19593,\n",
       "        14407, 20901,   184,  2692, 12118,  5759,  1204, 17176,  5745,  4380,\n",
       "         4167, 11300,  5800,  1424,  4167,  1398,  2176, 27750, 15624, 27863,\n",
       "          184,  2692,  1436,  4060,  1306,  2083,   144, 20910, 11741,   119,\n",
       "          112,   166,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81431f04-b18a-4fb5-b8a3-312bcc4ee876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='trained_models/job_results',\n",
    "    logging_dir='trained_models/job_logs',\n",
    "    metric_for_best_model = 'f1_score',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_job = Trainer(\n",
    "    model=Jobmodel,\n",
    "    tokenizer=tokenizer_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "282ef0ff-e6a7-414a-857c-8990d329cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/trainer.py:1325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1328\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1331\u001b[0m ):\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/trainer.py:1884\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1884\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1887\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/trainer.py:1916\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1915\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1916\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/threading.py:1089\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/threading.py:1109\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1110\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_job.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0c599-1e95-4c4a-9d49-ea71fdba9eb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. Multilingual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "465d4139-6061-4440-b809-ac4ab0a48f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# here applied a different tokenizer compared to the other 2 models\n",
    "multi_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "multilingual_model = \"bert-base-multilingual-cased\" \n",
    "Multi_Bertmodel =  BertForSequenceClassification.from_pretrained(multilingual_model, num_labels=gwa_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc2b4565-e9ae-463c-b651-99c27f2ccf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2bd0bdea745cdb59\n",
      "Found cached dataset csv (/home/user/jinqli/.cache/huggingface/datasets/csv/default-2bd0bdea745cdb59/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  23543\n",
      "length of the training set:  18834\n",
      "length of the test set:  2354\n",
      "length of the val set:  2355\n"
     ]
    }
   ],
   "source": [
    "# encoding data with corresponding tokenizer\n",
    "encoded_data = encode_data(de_path, multi_tokenizer, 'GWA Title')\n",
    "\n",
    "# print('An example of zipped task and label: \\n', encoded_data.__getitem__(10)) \n",
    "train_set,val_set,test_set = split_data(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3de4361-3232-47a3-97c8-c179b88905af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='trained_models/multibert_results',\n",
    "    logging_dir='trained_models/multibert_logs',\n",
    "    metric_for_best_model = 'f1_score',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_multibert = Trainer(\n",
    "    model=Multi_Bertmodel,\n",
    "    tokenizer=multi_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d41198-361d-458f-ba57-cc3919cd6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1480/1480 25:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>2.714472</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.088463</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>0.259023</td>\n",
       "      <td>7.873000</td>\n",
       "      <td>299.124000</td>\n",
       "      <td>2.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>2.196005</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>0.216565</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>0.427601</td>\n",
       "      <td>7.863600</td>\n",
       "      <td>299.483000</td>\n",
       "      <td>2.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>1.986812</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>0.257020</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>0.464119</td>\n",
       "      <td>8.154400</td>\n",
       "      <td>288.803000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>1.857215</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.304660</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>7.867000</td>\n",
       "      <td>299.353000</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>1.803832</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.331783</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>7.862100</td>\n",
       "      <td>299.539000</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>1.749402</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.364005</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>7.868400</td>\n",
       "      <td>299.299000</td>\n",
       "      <td>2.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>1.734157</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>7.848200</td>\n",
       "      <td>300.068000</td>\n",
       "      <td>2.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>1.729213</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.382370</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>0.524841</td>\n",
       "      <td>8.090700</td>\n",
       "      <td>291.076000</td>\n",
       "      <td>2.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.211800</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>1.736151</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.387682</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>0.527389</td>\n",
       "      <td>7.850500</td>\n",
       "      <td>299.980000</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>1.729934</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.393352</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>7.856900</td>\n",
       "      <td>299.737000</td>\n",
       "      <td>2.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>1.737821</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.402857</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>7.851300</td>\n",
       "      <td>299.950000</td>\n",
       "      <td>2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>1.729427</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>0.407679</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>0.528238</td>\n",
       "      <td>7.878000</td>\n",
       "      <td>298.933000</td>\n",
       "      <td>2.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>1.741030</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.408983</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>7.886700</td>\n",
       "      <td>298.604000</td>\n",
       "      <td>2.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.368800</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>1.740551</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.401061</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>8.171700</td>\n",
       "      <td>288.191000</td>\n",
       "      <td>2.325000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./multibert_results/checkpoint-500\n",
      "Configuration saved in ./multibert_results/checkpoint-500/config.json\n",
      "Model weights saved in ./multibert_results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./multibert_results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./multibert_results/checkpoint-500/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./multibert_results/checkpoint-1000\n",
      "Configuration saved in ./multibert_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./multibert_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./multibert_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./multibert_results/checkpoint-1000/special_tokens_map.json\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./multibert_results/checkpoint-1000 (score: 0.5286624203821656).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1480, training_loss=1.570254846521326, metrics={'train_runtime': 1516.2553, 'train_samples_per_second': 124.214, 'train_steps_per_second': 0.976, 'total_flos': 1.23924771650304e+16, 'train_loss': 1.570254846521326, 'epoch': 10.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_multibert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bc91661-3593-47bf-a4b7-6375995014b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./multibert_results/checkpoint-1000\n",
      "Configuration saved in ./multibert_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./multibert_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./multibert_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./multibert_results/checkpoint-1000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "best_ckpt_path = trainer_multibert.state.best_model_checkpoint\n",
    "trainer_multibert.save_model(best_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c152205-ae76-473f-bc30-aacc17faf769",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4. multilingual job model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "601531a8-16f4-4a95-990a-cc35c64f4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaConfig, XLMRobertaForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f48bffa-cfb7-4170-8bac-73d9bb6c4e65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /srv/scratch2/jinq/model_ep_30 were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /srv/scratch2/jinq/model_ep_30 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path = '/srv/scratch2/jinq/model_ep_30'\n",
    "\n",
    "# import the model\n",
    "multi_job_model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "# import the tokenizer \n",
    "SPECIAL_TOKENS_MAP_FILE = 'special_tokens_map.json'\n",
    "TOKENIZER_CONFIG_FILE = 'tokenizer_config.json'\n",
    "\n",
    "tokenizer_xlm = XLMRobertaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f736e50-f431-4351-b729-53d83bb756b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2bd0bdea745cdb59\n",
      "Found cached dataset csv (/home/user/jinqli/.cache/huggingface/datasets/csv/default-2bd0bdea745cdb59/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  23543\n",
      "length of the training set:  18834\n",
      "length of the test set:  2354\n",
      "length of the val set:  2355\n"
     ]
    }
   ],
   "source": [
    "# encoding data with corresponding tokenizer\n",
    "encoded_data = encode_data(de_path, tokenizer_xlm, 'GWA Title')\n",
    "\n",
    "# print('An example of zipped task and label: \\n', encoded_data.__getitem__(10)) \n",
    "train_set,val_set,test_set = split_data(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71941c88-e485-4e1d-9e5e-6c7c77d418fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    eval_steps = 100, # Evaluation and Save happens every 100 steps\n",
    "    save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    output_dir='trained_models/m4_multijob_results',\n",
    "    logging_dir='trained_models/m4_multijob_logs',\n",
    "    metric_for_best_model = 'f1_score',\n",
    "    load_best_model_at_end=True\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_multi_job = Trainer(\n",
    "    model=multi_job_model,\n",
    "    tokenizer=tokenizer_xlm,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a4613e6-188b-409a-95cf-004eadfb3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 18834\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1480\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1206, in forward\n    outputs = self.roberta(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 853, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 526, in forward\n    layer_outputs = layer_module(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 412, in forward\n    self_attention_outputs = self.attention(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 339, in forward\n    self_outputs = self.self(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 203, in forward\n    mixed_query_layer = self.query(hidden_states)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer_multi_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/trainer.py:1325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1328\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1331\u001b[0m ):\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/trainer.py:1884\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1884\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1887\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/trainer.py:1916\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1915\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1916\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    167\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:86\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 86\u001b[0m         output\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m     87\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1206, in forward\n    outputs = self.roberta(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 853, in forward\n    encoder_outputs = self.encoder(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 526, in forward\n    layer_outputs = layer_module(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 412, in forward\n    self_attention_outputs = self.attention(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 339, in forward\n    self_outputs = self.self(\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\", line 203, in forward\n    mixed_query_layer = self.query(hidden_states)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n    return F.linear(input, self.weight, self.bias)\nRuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n"
     ]
    }
   ],
   "source": [
    "trainer_multi_job.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736166e-5091-404d-b162-e8b510ecd6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b705d53-4c41-4a17-9b57-b6d6402f4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = trainer_multi_job.state.best_model_checkpoint\n",
    "trainer_multi_job.save_model(best_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f57c72-69b2-4643-a044-ccb6086f4769",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evaluate the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08f7612f-9777-414f-a9c6-e67e86e7a7be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8680411577224731, 'eval_accuracy': 0.5040339702760085, 'eval_balanced_accuracy': 0.3215903417617457, 'eval_f1_score': 0.5040339702760085, 'eval_recall': 0.5040339702760085, 'eval_precision': 0.5040339702760085, 'eval_hamming_loss': 0.5040339702760085, 'eval_runtime': 6.2722, 'eval_samples_per_second': 375.466, 'eval_steps_per_second': 3.029, 'epoch': 8.78}\n"
     ]
    }
   ],
   "source": [
    "## Comparing these two models\n",
    "metrics_gbert=trainer_gbert.evaluate()\n",
    "print(metrics_gbert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6de72fda-4a43-4ea1-bb25-44a149c4bb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8994202613830566, 'eval_accuracy': 0.48874734607218684, 'eval_balanced_accuracy': 0.29438654919459367, 'eval_f1_score': 0.48874734607218684, 'eval_recall': 0.48874734607218684, 'eval_precision': 0.48874734607218684, 'eval_hamming_loss': 0.48874734607218684, 'eval_runtime': 6.2803, 'eval_samples_per_second': 374.979, 'eval_steps_per_second': 3.025, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_job=trainer_job.evaluate()\n",
    "print(metrics_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4777085e-0416-406d-a727-966055282bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2355\n",
      "  Batch size = 128\n",
      "/home/user/jinqli/anaconda3/envs/new_env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7299339771270752, 'eval_accuracy': 0.5286624203821656, 'eval_balanced_accuracy': 0.3933516082476829, 'eval_f1_score': 0.5286624203821656, 'eval_recall': 0.5286624203821656, 'eval_precision': 0.5286624203821656, 'eval_hamming_loss': 0.5286624203821656, 'eval_runtime': 7.4651, 'eval_samples_per_second': 315.469, 'eval_steps_per_second': 2.545, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "metrics_multibert=trainer_multibert.evaluate()\n",
    "print(metrics_multibert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa200f3a-6989-4f2c-ab56-a318fdfa75c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
